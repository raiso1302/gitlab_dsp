{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Define gradient\n",
    "#w: model parameters value matrix\n",
    "#Xbar: variable value matrix\n",
    "#y: output vector\n",
    "class LR_GD:\n",
    "    def __init__(self):\n",
    "         #n rows 1 col.\n",
    "    #predicting function\n",
    "    \"\"\"\n",
    "    predicting function\n",
    "    X: shape [m_sample, n_features]\n",
    "    w: shape [n_features,1]\n",
    "    return [m_samples,1]\n",
    "    \"\"\"\n",
    "    def predict(self,w, X):\n",
    "        return np.dot(X, w)\n",
    "    #calculating cost function\n",
    "    def  cal_cost(self,w,X,Y):\n",
    "        m = len(Y)\n",
    "        cost = (1/2*m) * np.sum(np.square(Y - self.predict(w,X)))\n",
    "        return cost\n",
    "    #calculating gradient of cost function:\n",
    "    def cal_grad_cost(self, X, P, Y):\n",
    "        return np.dot(np.transpose(X),(P-Y))/(len(Y))\n",
    "\n",
    "    #standardize features\n",
    "    def standardize_features(self,X):\n",
    "        # standardize features\n",
    "        X_std = np.copy(X)\n",
    "        for it in range(X.shape[1]+1):\n",
    "            X_std[:, it] = (X[:, it] - X[:, it].mean()) / X[:, it].std()\n",
    "        return X_std\n",
    "    def optimize(self, X,Y,learning_rate,iterations):\n",
    "        #your code here to optimize w using X,Y\n",
    "        m = len(Y)\n",
    "        #standardize\n",
    "        X_std = self.standardize_features(X)\n",
    "        #adding matrix one to X\n",
    "        ones = np.ones((Y.shape[0],1))\n",
    "        X_add_one = np.concatenate((X_std,ones),axis=1)\n",
    "        #random initiate model parameter\n",
    "        self.w = np.random.randn(X_add_one.shape[1],1)\n",
    "\n",
    "        cost_history = np.zeros(iterations)\n",
    "        w_history = np.zeros((iterations,self.w.shape[0]))\n",
    "        # matrix zeros co so hang bang so lan it,\n",
    "        # so cot bang so features\n",
    "\n",
    "        for it in range(iterations):\n",
    "            predictions = self.predict(self.w, X_add_one)\n",
    "            #theta_new = theta - learning_rate*df/dtheta\n",
    "            self.w -= learning_rate*self.cal_grad_cost(X_add_one,predictions,Y)\n",
    "            wT = np.transpose(self.w)\n",
    "            w_history[it,:] = wT\n",
    "            cost_history[it]  = self.cal_cost(self.w,X,Y)\n",
    "            if ((it>0) & ((cost_history[(it - 1)] - cost_history[it])<1e-6)):\n",
    "                break\n",
    "\n",
    "        return self.w , cost_history, w_history\n",
    "    #def validate(self,X_test, Y_test):\n",
    "        #check the accuracy of the model\n",
    "#TODO: apply the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## feature engineering\n",
    "- learn the correlation of features\n",
    "- features reduction\n",
    "- one-hot encoding\n",
    "- standardize features\n",
    "- optimize output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#learn correlation\n",
    "#learn quantiles\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}